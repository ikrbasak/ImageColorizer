{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b534c07b-03a9-432a-8e61-3f9ef25c30f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Image Colorizer using Deep Learning\n",
    "## Introduction\n",
    "Image colorization is the process of adding color to a grayscale image. This can be done through a variety of methods, including manual methods such as painting or digital methods such as neural networks.\n",
    "\n",
    "One common approach to digital colorization is to use a convolutional neural network (CNN) trained on a dataset of color images. The network is trained to learn the relationships between the grayscale values of an image and the corresponding color values. Once trained, the network can then be used to colorize new grayscale images by using the learned relationships to predict the appropriate color values for each pixel.\n",
    "\n",
    "Another approach is using Generative Adversarial Networks (GANs) where a generator network learns to generate a colored image from a grayscale image and a discriminator network is trained to distinguish between real color images and the generated colored images. As the generator network improves, the discriminator network is no longer able to distinguish between the real and generated images, resulting in a high-quality colorization.\n",
    "\n",
    "## Goal\n",
    "The goal is to develop a deep learning-based image colorization system using a cGAN architecture that can accurately and realistically colorize grayscale images while preserving fine details and textures in the final output. The aim is to achieve this by training the generator network to learn the relationships between the grayscale input image and the corresponding color output, and the discriminator network to effectively distinguish between the generated colored images and real color images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b18602-2824-4c82-85c4-906e88ff93df",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network (GAN)\n",
    "## What is GAN?\n",
    "A Generative Adversarial Network (GAN) is a type of deep learning model that is used for generative tasks, such as image generation and image colorization. It is composed of two main parts: a generator network and a discriminator network.\n",
    "\n",
    "The generator network is trained to generate new data samples that are similar to the training data. For example, it can generate new images that resemble the training images. The generator network takes a random input, called a noise vector, and maps it to a sample of the target data distribution.\n",
    "\n",
    "The discriminator network is trained to distinguish between real data samples and the generated samples produced by the generator network. It takes an input (real or generated) and output a probability of whether the input is real or fake.\n",
    "\n",
    "The generator and discriminator networks are trained together in an adversarial manner, where the generator is trying to produce samples that can fool the discriminator and the discriminator is trying to correctly identify whether the input is real or fake. Through this competition, the generator learns to produce more realistic samples, while the discriminator becomes better at identifying fake samples. Eventually, the generator produces samples that are indistinguishable from real data and the GAN has successfully learned the target data distribution.\n",
    "\n",
    "> This project implements a `Conditional Wasserstein GAN`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300f72a-f7cd-41fd-ab0b-e939f5dd4be3",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fec98e-a5f1-4dbf-931d-b1b80c2a1426",
   "metadata": {},
   "source": [
    "## Importing necessary modules/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497b5ff-a21b-4751-97f4-b0033fa8a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from skimage.color import lab2rgb\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F  # noqa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torchvision.models.inception import inception_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3f3b6-cdef-4583-8675-a781020e869a",
   "metadata": {},
   "source": [
    "## Setting module/packange environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd862da-c666-4622-9e76-57729566eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "matplotlib.style.use(\"seaborn-pastel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96569c5b-740f-4073-a48d-48e68d03529e",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Used [Image Colorization Dataset by Shravankumar Shetty](https://www.kaggle.com/datasets/shravankumar9892/image-colorization) from (Kaggle)[https://www.kaggle.com/]. The dataset consists 25000 images of [LAB](https://www.xrite.com/blog/lab-color-space) color space. \n",
    "\n",
    "The dataset is very large and only first 10000 images has been used for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375bc20-fc83-486c-925a-bdafc98834da",
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_image_path = \"/kaggle/input/image-colorization/ab/ab/ab1.npy\"\n",
    "L_image_path = \"/kaggle/input/image-colorization/l/gray_scale.npy\"\n",
    "\n",
    "AB_image_df = np.load(AB_image_path)\n",
    "L_image_df = np.load(L_image_path)[: AB_image_df.shape[0]]\n",
    "\n",
    "print(f\"Total {AB_image_df.shape[0]} Color images of shape {AB_image_df.shape[1:]}\")\n",
    "print(f\"Total {L_image_df.shape[0]} Color images of shape {L_image_df.shape[1:]}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75cb8a1-540d-48cd-a1a3-d78e0d207e6d",
   "metadata": {},
   "source": [
    "**Utility function to convert images to RGB from individual L and A&B component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f3fd3-04d1-4777-9642-2d1970f0bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LAB_to_RGB(L_img, AB_img):\n",
    "    L_img = L_img * 100\n",
    "    AB_img = (AB_img - 0.5) * 128 * 2\n",
    "    LAB_img = torch.cat([L_img, AB_img], dim=2).numpy()\n",
    "    RGB_images = []\n",
    "    for img in LAB_img:\n",
    "        img_RGB = lab2rgb(img)\n",
    "        RGB_images.append(img_RGB)\n",
    "    return np.stack(RGB_images, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f081a-5885-4240-8d05-e535d19f973b",
   "metadata": {},
   "source": [
    "**Plot random images to show the images in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a5893-cccd-4e98-b08c-53623cc2e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = random.randint(20, 50)\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "for i in range(n + 1, n + 17, 2):\n",
    "    plt.subplot(4, 4, (i - n))\n",
    "    img = np.zeros((224, 224, 3))\n",
    "    img[:, :, 0] = L_image_df[i]\n",
    "    plt.title(\"B&W\")\n",
    "    plt.imshow(lab2rgb(img))\n",
    "\n",
    "    plt.subplot(4, 4, (i + 1 - n))\n",
    "    img[:, :, 1:] = AB_image_df[i]\n",
    "    img = img.astype(\"uint8\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n",
    "    plt.title(\"Colored\")\n",
    "    plt.imshow(img)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71186577-49b0-44da-ad2e-c48e8910145c",
   "metadata": {},
   "source": [
    "**Convert the dataset as pytorch tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d39d4-617b-402f-9da9-6e73f9e1860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageColorizationDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        L = np.array(self.dataset[0][idx]).reshape((224, 224, 1))\n",
    "        L = transforms.ToTensor()(L)\n",
    "\n",
    "        AB = np.array(self.dataset[1][idx])\n",
    "        AB = transforms.ToTensor()(AB)\n",
    "\n",
    "        return AB, L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84020ad7-ea11-4ace-a532-b15a898e612e",
   "metadata": {},
   "source": [
    "**Build the data generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b21eb9-18b7-4852-b774-5a3f67d36892",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "split = 0.3\n",
    "train_size = int(AB_image_df.shape[0] * (1 - split))\n",
    "test_size = int(AB_image_df.shape[0] * split)\n",
    "\n",
    "train_dataset = ImageColorizationDataset(dataset=(L_image_df[:train_size], AB_image_df[:train_size]))\n",
    "test_dataset = ImageColorizationDataset(dataset=(L_image_df[-test_size:], AB_image_df[-test_size:]))\n",
    "\n",
    "print(f\"Train dataset has {len(train_dataset)} images\")\n",
    "print(f\"Test dataset has {len(test_dataset)} images\")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4e226-b534-4d65-9233-3bd4eb69f6c0",
   "metadata": {},
   "source": [
    "## Generator\n",
    "The generator is a UNet with ResBlock for Semantic Segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8176f-02c2-496e-8692-d34ab34be1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                stride=stride,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                stride=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.identity_map = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.clone().detach()\n",
    "        out = self.layer(x)\n",
    "        residual = self.identity_map(inputs)\n",
    "        skip = out + residual\n",
    "        return self.relu(skip)\n",
    "\n",
    "\n",
    "class DownSampleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(nn.MaxPool2d(2), ResBlock(in_channels, out_channels))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.layer(inputs)\n",
    "\n",
    "\n",
    "class UpSampleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.res_block = ResBlock(in_channels + out_channels, out_channels)\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.upsample(inputs)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.encoding_layer1_ = ResBlock(input_channel, 64)\n",
    "        self.encoding_layer2_ = DownSampleConv(64, 128)\n",
    "        self.encoding_layer3_ = DownSampleConv(128, 256)\n",
    "        self.bridge = DownSampleConv(256, 512)\n",
    "        self.decoding_layer3_ = UpSampleConv(512, 256)\n",
    "        self.decoding_layer2_ = UpSampleConv(256, 128)\n",
    "        self.decoding_layer1_ = UpSampleConv(128, 64)\n",
    "        self.output = nn.Conv2d(64, output_channel, kernel_size=1)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        e1 = self.encoding_layer1_(inputs)\n",
    "        e1 = self.dropout(e1)\n",
    "        e2 = self.encoding_layer2_(e1)\n",
    "        e2 = self.dropout(e2)\n",
    "        e3 = self.encoding_layer3_(e2)\n",
    "        e3 = self.dropout(e3)\n",
    "\n",
    "        bridge = self.bridge(e3)\n",
    "        bridge = self.dropout(bridge)\n",
    "\n",
    "        d3 = self.decoding_layer3_(bridge, e3)\n",
    "        d2 = self.decoding_layer2_(d3, e2)\n",
    "        d1 = self.decoding_layer1_(d2, e1)\n",
    "\n",
    "        output = self.output(d1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5b1e6-f529-4648-839d-05ed39d74365",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator(1, 2).to(device)\n",
    "summary(model, (1, 224, 224), batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea3544-b250-4b0e-905e-410514b1a4df",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "The discriminator is a standard Convolutional Neural Network (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65d8f0-1592-4729-9e27-6cf3edb20312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, ab, l):\n",
    "        img_input = torch.cat((ab, l), 1)\n",
    "        output = self.model(img_input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32286389-d00f-400e-b828-7fb02e2d6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Discriminator(3).to(device)\n",
    "summary(model, [(2, 224, 224), (1, 224, 224)], batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149eaa5-2998-48d0-8a26-556eaeb02da1",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db63805-4d8d-44da-b7e0-fbb98161c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def display_progress(cond, real, fake, current_epoch=0, figsize=(20, 15)):\n",
    "    cond = cond.detach().cpu().permute(1, 2, 0)\n",
    "    real = real.detach().cpu().permute(1, 2, 0)\n",
    "    fake = fake.detach().cpu().permute(1, 2, 0)\n",
    "\n",
    "    images = [cond, real, fake]\n",
    "    titles = [\"input\", \"real\", \"generated\"]\n",
    "    print(f\"Epoch: {current_epoch}\")\n",
    "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "    for idx, img in enumerate(images):\n",
    "        if idx == 0:\n",
    "            ab = torch.zeros((224, 224, 2))\n",
    "            img = torch.cat([images[0] * 100, ab], dim=2).numpy()\n",
    "            imgan = lab2rgb(img)\n",
    "        else:\n",
    "            imgan = LAB_to_RGB(images[0], img)\n",
    "        ax[idx].imshow(imgan)\n",
    "        ax[idx].axis(\"off\")\n",
    "    for idx, title in enumerate(titles):\n",
    "        ax[idx].set_title(\"{}\".format(title))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class ConditionalWGAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        learning_rate=0.0002,\n",
    "        lambda_recon=100,\n",
    "        display_step=10,\n",
    "        lambda_gp=10,\n",
    "        lambda_r1=10,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.display_step = display_step\n",
    "\n",
    "        self.generator = Generator(in_channels, out_channels)\n",
    "        self.discriminator = Discriminator(in_channels + out_channels)\n",
    "        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n",
    "        self.optimizer_C = optim.Adam(self.discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n",
    "        self.lambda_recon = lambda_recon\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.lambda_r1 = lambda_r1\n",
    "        self.recon_criterion = nn.L1Loss()\n",
    "        self.generator_losses, self.discriminator_losses = [], []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [self.optimizer_C, self.optimizer_G]\n",
    "\n",
    "    def generator_step(self, real_images, conditioned_images):\n",
    "        self.optimizer_G.zero_grad()\n",
    "        fake_images = self.generator(conditioned_images)\n",
    "        recon_loss = self.recon_criterion(fake_images, real_images)\n",
    "        recon_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        self.generator_losses += [recon_loss.item()]\n",
    "\n",
    "    def discriminator_step(self, real_images, conditioned_images):\n",
    "        self.optimizer_C.zero_grad()\n",
    "        fake_images = self.generator(conditioned_images)\n",
    "        fake_logits = self.discriminator(fake_images, conditioned_images)\n",
    "        real_logits = self.discriminator(real_images, conditioned_images)\n",
    "\n",
    "        loss_C = real_logits.mean() - fake_logits.mean()\n",
    "\n",
    "        alpha = torch.rand(real_images.size(0), 1, 1, 1, requires_grad=True)\n",
    "        alpha = alpha.to(device)\n",
    "        interpolated = (alpha * real_images + (1 - alpha) * fake_images.detach()).requires_grad_(True)\n",
    "\n",
    "        interpolated_logits = self.discriminator(interpolated, conditioned_images)\n",
    "\n",
    "        grad_outputs = torch.ones_like(interpolated_logits, dtype=torch.float32, requires_grad=True)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=interpolated_logits,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        gradients = gradients.view(len(gradients), -1)\n",
    "        gradients_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        loss_C += self.lambda_gp * gradients_penalty\n",
    "\n",
    "        r1_reg = gradients.pow(2).sum(1).mean()\n",
    "        loss_C += self.lambda_r1 * r1_reg\n",
    "\n",
    "        loss_C.backward()\n",
    "        self.optimizer_C.step()\n",
    "        self.discriminator_losses += [loss_C.item()]\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real, condition = batch\n",
    "        if optimizer_idx == 0:\n",
    "            self.discriminator_step(real, condition)\n",
    "        elif optimizer_idx == 1:\n",
    "            self.generator_step(real, condition)\n",
    "        gen_mean = sum(self.generator_losses[-self.display_step :]) / self.display_step\n",
    "        crit_mean = sum(self.discriminator_losses[-self.display_step :]) / self.display_step\n",
    "        if self.current_epoch % self.display_step == 0 and batch_idx == 0 and optimizer_idx == 1:\n",
    "            fake = self.generator(condition).detach()\n",
    "            print(f\"Epoch {self.current_epoch} : Generator loss: {gen_mean}, discriminator loss: {crit_mean}\")\n",
    "            display_progress(condition[0], real[0], fake[0], self.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5a64c-8cab-4d58-b50c-89135f34bb3e",
   "metadata": {},
   "source": [
    "## Build and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe82b0-9c2d-4f39-901a-088b019fc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "cwgan = ConditionalWGAN(in_channels=1, out_channels=2, learning_rate=2e-4, lambda_recon=100, display_step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626c87b-d3f6-4767-b2e0-a237061d65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=150, gpus=-1)\n",
    "trainer.fit(cwgan, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120164b-38d2-4ee0-8898-97420b38a256",
   "metadata": {},
   "source": [
    "## Model inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bdc9c9-4d75-43e4-b6e3-3c66a8cb1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 60))\n",
    "idx = 40\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    real, condition = batch\n",
    "    pred = cwgan.generator(condition).detach().squeeze().permute(1, 2, 0)\n",
    "    condition = condition.detach().squeeze(0).permute(1, 2, 0)\n",
    "    real = real.detach().squeeze(0).permute(1, 2, 0)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.subplot(6, 3, idx)\n",
    "    plt.grid(False)\n",
    "\n",
    "    ab = torch.zeros((224, 224, 2))\n",
    "    img = torch.cat([condition * 100, ab], dim=2).numpy()\n",
    "    imgan = lab2rgb(img)\n",
    "    plt.imshow(imgan)\n",
    "    plt.title(\"Input\")\n",
    "\n",
    "    plt.subplot(6, 3, idx + 1)\n",
    "\n",
    "    ab = torch.zeros((224, 224, 2))\n",
    "    imgan = LAB_to_RGB(condition, real)\n",
    "    plt.imshow(imgan)\n",
    "    plt.title(\"Real\")\n",
    "\n",
    "    plt.subplot(6, 3, idx + 2)\n",
    "    imgan = LAB_to_RGB(condition, pred)\n",
    "    plt.title(\"Generated\")\n",
    "    plt.imshow(imgan)\n",
    "    idx += 3\n",
    "    if idx >= 18:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8357811-f4a8-416f-9376-c7eb53b3c90c",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2bf89c-8ab0-437c-9743-8f08cc250a55",
   "metadata": {},
   "source": [
    "**Inception Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe0166-a1de-4fdc-8ab2-e3fdb60bc1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "cwgan.generator.eval()\n",
    "all_preds = []\n",
    "all_real = []\n",
    "\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    real, condition = batch\n",
    "    pred = cwgan.generator(condition).detach()\n",
    "    Lab = torch.cat([condition, pred], dim=1).numpy()\n",
    "    Lab_real = torch.cat([condition, real], dim=1).numpy()\n",
    "    all_preds.append(Lab.squeeze())\n",
    "    all_real.append(Lab_real.squeeze())\n",
    "    if batch_idx == 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc20562-97bc-4b49-998a-1e863c05eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionScore:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.inception = inception_v3(pretrained=True, transform_input=False).to(self.device)\n",
    "        self.inception.eval()\n",
    "\n",
    "    def calculate_is(self, generated_images):\n",
    "        generated_images = generated_images.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_features = self.inception(generated_images.view(-1, 3, 224, 224))\n",
    "\n",
    "        generated_features = generated_features.view(generated_features.size(0), -1)\n",
    "        p = F.softmax(generated_features, dim=1)\n",
    "\n",
    "        kl = p * (torch.log(p) - torch.log(torch.tensor(1.0 / generated_features.size(1)).to(self.device)))\n",
    "        kl = kl.sum(dim=1)\n",
    "\n",
    "        return kl.mean().item(), kl.std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff88d0b4-b7b9-41b7-88f4-3ed1159679e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = [\"cuda\", \"cpu\"][0]\n",
    "is_calculator = InceptionScore(device)\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_preds = torch.tensor(all_preds).float()\n",
    "\n",
    "all_real = np.concatenate(all_real, axis=0)\n",
    "all_real = torch.tensor(all_real).float()\n",
    "\n",
    "is_model = InceptionScore(device)\n",
    "\n",
    "mean_real, std_real = is_model.calculate_is(all_real)\n",
    "mean_is, std_is = is_model.calculate_is(all_preds)\n",
    "\n",
    "print(\"Inception Score of real images: mean: {:.4f}, std: {:.4f}\".format(mean_real, std_real))\n",
    "print(\"Inception Score of fake images: mean: {:.4f}, std: {:.4f}\".format(mean_is, std_is))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9922eff0-d74a-4cde-ad51-bc483662dc4b",
   "metadata": {},
   "source": [
    "**Frechet Inception Distance (FID)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be607174-7d6f-4f69-9e16-38aaee9afd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FID:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.inception = inception_v3(pretrained=True, transform_input=False).to(self.device)\n",
    "        self.inception.eval()\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "\n",
    "    def calculate_fid(self, real_images, generated_images):\n",
    "        real_images = real_images.to(self.device)\n",
    "        generated_images = generated_images.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            real_features = self.inception(real_images.view(-1, 3, 224, 224))\n",
    "            generated_features = self.inception(generated_images.view(-1, 3, 224, 224))\n",
    "\n",
    "        real_features = real_features.view(real_features.size(0), -1)\n",
    "        generated_features = generated_features.view(generated_features.size(0), -1)\n",
    "\n",
    "        if self.mu is None:\n",
    "            self.mu = real_features.mean(dim=0)\n",
    "\n",
    "        if self.sigma is None:\n",
    "            self.sigma = real_features.std(dim=0)\n",
    "\n",
    "        real_mu = real_features.mean(dim=0)\n",
    "        real_sigma = real_features.std(dim=0)\n",
    "\n",
    "        generated_mu = generated_features.mean(dim=0)\n",
    "        generated_sigma = generated_features.std(dim=0)\n",
    "\n",
    "        mu_diff = real_mu - generated_mu\n",
    "        sigma_diff = real_sigma - generated_sigma\n",
    "\n",
    "        fid = mu_diff.pow(2).sum() + (self.sigma - generated_sigma).pow(2).sum() + (self.mu - generated_mu).pow(2).sum()\n",
    "        return fid.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72de7ff-19c9-42a8-a889-436a2c85bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = [\"cuda\", \"cpu\"][0]\n",
    "fid_calculator = FID(device)\n",
    "\n",
    "fid_value = fid_calculator.calculate_fid(all_real, all_preds)\n",
    "print(\"FID: {:.4f}\".format(fid_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1efd7e-3c96-4bf3-a2ba-d63de274797b",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Two methods to enhance the performance of the model for image colorization are by utilizing the Wasserstein GAN (WGAN) and a U-Net architecture that incorporates residual blocks. The WGAN technique utilizes the Wasserstein distance metric during the training of the generator and discriminator, which can lead to a more stable training process and generate more realistic output. A U-Net structure, which is particularly efficient for image segmentation tasks, combined with the use of residual blocks allows the network to capture fine details of the input image, which is essential for colorization tasks. This can enhance the stability and the capability to learn fine details of the input image, resulting in more realistic output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b039e718-7af0-44e4-916b-b37cf1940350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
