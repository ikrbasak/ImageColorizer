{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497b5ff-a21b-4751-97f4-b0033fa8a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import PIL\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "from scipy.stats import entropy\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F  # noqa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision.models.inception import inception_v3\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd862da-c666-4622-9e76-57729566eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "matplotlib.style.use(\"seaborn-pastel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375bc20-fc83-486c-925a-bdafc98834da",
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_image_path = \"/kaggle/input/image-colorization/ab/ab/ab1.npy\"\n",
    "L_image_path = \"/kaggle/input/image-colorization/l/gray_scale.npy\"\n",
    "\n",
    "AB_image_df = np.load(AB_image_path)\n",
    "L_image_df = np.load(L_image_path)[: AB_image_df.shape[0]]\n",
    "\n",
    "print(f\"Total {AB_image_df.shape[0]} Color images of shape {AB_image_df.shape[1:]}\")\n",
    "print(f\"Total {L_image_df.shape[0]} Color images of shape {L_image_df.shape[1:]}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f3fd3-04d1-4777-9642-2d1970f0bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LAB_to_RGB(L_img, AB_img):\n",
    "    L_img = L_img * 100\n",
    "    AB_img = (AB_img - 0.5) * 128 * 2\n",
    "    LAB_img = torch.cat([L_img, AB_img], dim=2).numpy()\n",
    "    RGB_images = []\n",
    "    for img in LAB_img:\n",
    "        img_RGB = lab2rgb(img)\n",
    "        RGB_images.append(img_RGB)\n",
    "    return np.stack(RGB_images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a5893-cccd-4e98-b08c-53623cc2e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = random.randint(20, 50)\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "for i in range(n + 1, n + 17, 2):\n",
    "    plt.subplot(4, 4, (i - n))\n",
    "    img = np.zeros((224, 224, 3))\n",
    "    img[:, :, 0] = L_image_df[i]\n",
    "    plt.title(\"B&W\")\n",
    "    plt.imshow(lab2rgb(img))\n",
    "\n",
    "    plt.subplot(4, 4, (i + 1 - n))\n",
    "    img[:, :, 1:] = AB_image_df[i]\n",
    "    img = img.astype(\"uint8\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n",
    "    plt.title(\"Colored\")\n",
    "    plt.imshow(img)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d39d4-617b-402f-9da9-6e73f9e1860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageColorizationDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        L = np.array(self.dataset[0][idx]).reshape((224, 224, 1))\n",
    "        L = transforms.ToTensor()(L)\n",
    "\n",
    "        AB = np.array(self.dataset[1][idx])\n",
    "        AB = transforms.ToTensor()(AB)\n",
    "\n",
    "        return AB, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b21eb9-18b7-4852-b774-5a3f67d36892",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "split = 0.3\n",
    "train_size = int(AB_image_df.shape[0] * (1 - split))\n",
    "test_size = int(AB_image_df.shape[0] * split)\n",
    "\n",
    "train_dataset = ImageColorizationDataset(dataset=(L_image_df[:train_size], AB_image_df[:train_size]))\n",
    "test_dataset = ImageColorizationDataset(dataset=(L_image_df[-test_size:], AB_image_df[-test_size:]))\n",
    "\n",
    "print(f\"Train dataset has {len(train_dataset)} images\")\n",
    "print(f\"Test dataset has {len(test_dataset)} images\")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8176f-02c2-496e-8692-d34ab34be1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.identity_map = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.clone().detach()\n",
    "        out = self.layer(x)\n",
    "        residual = self.identity_map(inputs)\n",
    "        skip = out + residual\n",
    "        return self.relu(skip)\n",
    "\n",
    "\n",
    "class DownSampleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(nn.MaxPool2d(2), ResBlock(in_channels, out_channels))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.layer(inputs)\n",
    "\n",
    "\n",
    "class UpSampleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.res_block = ResBlock(in_channels + out_channels, out_channels)\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.upsample(inputs)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.encoding_layer1_ = ResBlock(input_channel, 64)\n",
    "        self.encoding_layer2_ = DownSampleConv(64, 128)\n",
    "        self.encoding_layer3_ = DownSampleConv(128, 256)\n",
    "        self.bridge = DownSampleConv(256, 512)\n",
    "        self.decoding_layer3_ = UpSampleConv(512, 256)\n",
    "        self.decoding_layer2_ = UpSampleConv(256, 128)\n",
    "        self.decoding_layer1_ = UpSampleConv(128, 64)\n",
    "        self.output = nn.Conv2d(64, output_channel, kernel_size=1)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        e1 = self.encoding_layer1_(inputs)\n",
    "        e1 = self.dropout(e1)\n",
    "        e2 = self.encoding_layer2_(e1)\n",
    "        e2 = self.dropout(e2)\n",
    "        e3 = self.encoding_layer3_(e2)\n",
    "        e3 = self.dropout(e3)\n",
    "\n",
    "        bridge = self.bridge(e3)\n",
    "        bridge = self.dropout(bridge)\n",
    "\n",
    "        d3 = self.decoding_layer3_(bridge, e3)\n",
    "        d2 = self.decoding_layer2_(d3, e2)\n",
    "        d1 = self.decoding_layer1_(d2, e1)\n",
    "\n",
    "        output = self.output(d1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5b1e6-f529-4648-839d-05ed39d74365",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator(1, 2).to(device)\n",
    "summary(model, (1, 224, 224), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65d8f0-1592-4729-9e27-6cf3edb20312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, ab, l):\n",
    "        img_input = torch.cat((ab, l), 1)\n",
    "        output = self.model(img_input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32286389-d00f-400e-b828-7fb02e2d6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Discriminator(3).to(device)\n",
    "summary(model, [(2, 224, 224), (1, 224, 224)], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db63805-4d8d-44da-b7e0-fbb98161c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def display_progress(cond, real, fake, current_epoch=0, figsize=(20, 15)):\n",
    "    cond = cond.detach().cpu().permute(1, 2, 0)\n",
    "    real = real.detach().cpu().permute(1, 2, 0)\n",
    "    fake = fake.detach().cpu().permute(1, 2, 0)\n",
    "\n",
    "    images = [cond, real, fake]\n",
    "    titles = [\"input\", \"real\", \"generated\"]\n",
    "    print(f\"Epoch: {current_epoch}\")\n",
    "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "    for idx, img in enumerate(images):\n",
    "        if idx == 0:\n",
    "            ab = torch.zeros((224, 224, 2))\n",
    "            img = torch.cat([images[0] * 100, ab], dim=2).numpy()\n",
    "            imgan = lab2rgb(img)\n",
    "        else:\n",
    "            imgan = LAB_to_RGB(images[0], img)\n",
    "        ax[idx].imshow(imgan)\n",
    "        ax[idx].axis(\"off\")\n",
    "    for idx, title in enumerate(titles):\n",
    "        ax[idx].set_title(\"{}\".format(title))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class ConditionalWGAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            learning_rate=0.0002,\n",
    "            lambda_recon=100,\n",
    "            display_step=10,\n",
    "            lambda_gp=10,\n",
    "            lambda_r1=10,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.display_step = display_step\n",
    "\n",
    "        self.generator = Generator(in_channels, out_channels)\n",
    "        self.discriminator = Discriminator(in_channels + out_channels)\n",
    "        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n",
    "        self.optimizer_C = optim.Adam(self.discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n",
    "        self.lambda_recon = lambda_recon\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.lambda_r1 = lambda_r1\n",
    "        self.recon_criterion = nn.L1Loss()\n",
    "        self.generator_losses, self.discriminator_losses = [], []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [self.optimizer_C, self.optimizer_G]\n",
    "\n",
    "    def generator_step(self, real_images, conditioned_images):\n",
    "        self.optimizer_G.zero_grad()\n",
    "        fake_images = self.generator(conditioned_images)\n",
    "        recon_loss = self.recon_criterion(fake_images, real_images)\n",
    "        recon_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        self.generator_losses += [recon_loss.item()]\n",
    "\n",
    "    def discriminator_step(self, real_images, conditioned_images):\n",
    "        self.optimizer_C.zero_grad()\n",
    "        fake_images = self.generator(conditioned_images)\n",
    "        fake_logits = self.discriminator(fake_images, conditioned_images)\n",
    "        real_logits = self.discriminator(real_images, conditioned_images)\n",
    "\n",
    "        loss_C = real_logits.mean() - fake_logits.mean()\n",
    "\n",
    "        alpha = torch.rand(real_images.size(0), 1, 1, 1, requires_grad=True)\n",
    "        alpha = alpha.to(device)\n",
    "        interpolated = (alpha * real_images + (1 - alpha) * fake_images.detach()).requires_grad_(True)\n",
    "\n",
    "        interpolated_logits = self.discriminator(interpolated, conditioned_images)\n",
    "\n",
    "        grad_outputs = torch.ones_like(interpolated_logits, dtype=torch.float32, requires_grad=True)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=interpolated_logits,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        gradients = gradients.view(len(gradients), -1)\n",
    "        gradients_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        loss_C += self.lambda_gp * gradients_penalty\n",
    "\n",
    "        r1_reg = gradients.pow(2).sum(1).mean()\n",
    "        loss_C += self.lambda_r1 * r1_reg\n",
    "\n",
    "        loss_C.backward()\n",
    "        self.optimizer_C.step()\n",
    "        self.discriminator_losses += [loss_C.item()]\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real, condition = batch\n",
    "        if optimizer_idx == 0:\n",
    "            self.discriminator_step(real, condition)\n",
    "        elif optimizer_idx == 1:\n",
    "            self.generator_step(real, condition)\n",
    "        gen_mean = sum(self.generator_losses[-self.display_step:]) / self.display_step\n",
    "        crit_mean = sum(self.discriminator_losses[-self.display_step:]) / self.display_step\n",
    "        if self.current_epoch % self.display_step == 0 and batch_idx == 0 and optimizer_idx == 1:\n",
    "            fake = self.generator(condition).detach()\n",
    "            print(f\"Epoch {self.current_epoch} : Generator loss: {gen_mean}, discriminator loss: {crit_mean}\")\n",
    "            display_progress(condition[0], real[0], fake[0], self.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe82b0-9c2d-4f39-901a-088b019fc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "cwgan = ConditionalWGAN(in_channels=1, out_channels=2, learning_rate=2e-4, lambda_recon=100, display_step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626c87b-d3f6-4767-b2e0-a237061d65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=150, gpus=-1)\n",
    "trainer.fit(cwgan, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bdc9c9-4d75-43e4-b6e3-3c66a8cb1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 60))\n",
    "idx = 40\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    real, condition = batch\n",
    "    pred = cwgan.generator(condition).detach().squeeze().permute(1, 2, 0)\n",
    "    condition = condition.detach().squeeze(0).permute(1, 2, 0)\n",
    "    real = real.detach().squeeze(0).permute(1, 2, 0)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.subplot(6, 3, idx)\n",
    "    plt.grid(False)\n",
    "\n",
    "    ab = torch.zeros((224, 224, 2))\n",
    "    img = torch.cat([condition * 100, ab], dim=2).numpy()\n",
    "    imgan = lab2rgb(img)\n",
    "    plt.imshow(imgan)\n",
    "    plt.title('Input')\n",
    "\n",
    "    plt.subplot(6, 3, idx + 1)\n",
    "\n",
    "    ab = torch.zeros((224, 224, 2))\n",
    "    imgan = LAB_to_RGB(condition, real)\n",
    "    plt.imshow(imgan)\n",
    "    plt.title('Real')\n",
    "\n",
    "    plt.subplot(6, 3, idx + 2)\n",
    "    imgan = LAB_to_RGB(condition, pred)\n",
    "    plt.title('Generated')\n",
    "    plt.imshow(imgan)\n",
    "    idx += 3\n",
    "    if idx >= 18:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
